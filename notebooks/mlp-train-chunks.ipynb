{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Training in Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Using tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2048, activation='relu', input_dim=5000))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "size = 1281167\n",
    "chunk_size = 60000\n",
    "num_iters = math.ceil(size/chunk_size)\n",
    "\n",
    "train_file = \"f-selected/shuf_imagenet_200_vgg19_mrmr_5000_train.csv\"\n",
    "test_file = \"f-selected/shuf_imagenet_200_vgg19_mrmr_5000_val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"f-selected/shuf_imagenet_200_vgg19_mrmr_5000_train.csv\", 'r')\n",
    "header = f.readline()\n",
    "f.close\n",
    "\n",
    "label = header.strip('\\n').split(',')[-1]\n",
    "labels = [str(l) for l in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for epoch in range(10000): # epochs\n",
    "    train = pd.read_csv(train_file, iterator=True, chunksize=chunk_size)\n",
    "\n",
    "    for iter_ in range(num_iters):\n",
    "        chunk = train.get_chunk()\n",
    "        \n",
    "        if (iter_%10) == 0:        \n",
    "            train_chunk, val_chunk = train_test_split(chunk, test_size=0.1)\n",
    "        else:\n",
    "            train_chunk = chunk.copy()\n",
    "        \n",
    "        X_train = train_chunk.loc[:, chunk.columns != label].values\n",
    "        label_col = train_chunk[label].apply(lambda x: str(int(x))).astype('category',categories=labels)\n",
    "        y_train = pd.get_dummies(label_col).values\n",
    "        \n",
    "        model.train_on_batch(X_train, y_train)\n",
    "        \n",
    "        if (iter_%10) == 0:\n",
    "            X_val = val_chunk.loc[:, chunk.columns != label].values        \n",
    "            val_label_col = val_chunk[label].apply(lambda x: str(int(x))).astype('category',categories=labels)\n",
    "            y_val = pd.get_dummies(val_label_col).values \n",
    "            \n",
    "            acc = model.evaluate(X_val, y_val)\n",
    "            print(\"Iter: {}\\tEpoch: {}\\tVal acc: {}\".format(iter_+1, epoch+1, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "test_X = test.loc[:, chunk.columns != label].values\n",
    "test_label_col = test[label].apply(lambda x: str(int(x))).astype('category',categories=labels)\n",
    "test_y = pd.get_dummies(test_label_col).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "size = 1281167\n",
    "chunk_size = 50000\n",
    "num_iters = math.ceil(size/chunk_size)\n",
    "\n",
    "train_file = \"f-selected/shuf_imagenet_200_vgg19_mrmr_5000_train.csv\"\n",
    "test_file = \"f-selected/shuf_imagenet_200_vgg19_mrmr_5000_val.csv\"\n",
    "\n",
    "epochs = 10\n",
    "hidden_units = 2048\n",
    "num_iters = math.ceil(size/chunk_size)\n",
    "\n",
    "labels = [str(l) for l in range(1000)]\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(hidden_units, hidden_units), learning_rate_init=0.005,\n",
    "                    max_iter=1, shuffle=True, verbose=True, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    train = pd.read_csv(train_file, iterator=True, chunksize=chunk_size)\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        \n",
    "        chunk = train.get_chunk()\n",
    "        X_train = chunk.loc[:, chunk.columns != label].values\n",
    "        y_train = chunk[label].apply(lambda x: str(int(x))).values\n",
    "        \n",
    "        mlp = mlp.partial_fit(X_train, y_train, classes=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "test_X = test.loc[:, chunk.columns != label].values\n",
    "test_y = test[label].apply(lambda x: str(int(x))).values\n",
    "\n",
    "pred_test = mlp.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(test_y, pred_test)\n",
    "\n",
    "print(\"Test accuracy {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top-5 accuracy\n",
    "n = 5\n",
    "probs = mlp.predict_proba(test_X)\n",
    "best_n = np.argsort(probs, axis=1)[:,-n:]\n",
    "\n",
    "tp = 0\n",
    "for best_predictions, y in zip(best_n, test_y):\n",
    "    pred = [mlp.classes_[x] for x in best_predictions]\n",
    "    if y in pred:\n",
    "        tp += 1\n",
    "        \n",
    "top5_accuracty = float(tp / len(test_y))\n",
    "\n",
    "print(\"Test Top-5 accuracy {}\".format(top5_accuracty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dump\n",
    "from joblib import dump\n",
    "dump(mlp, 'sklearn-mlp-v1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model load\n",
    "from joblib import load\n",
    "mlp = load('sklearn-mlp-v1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
